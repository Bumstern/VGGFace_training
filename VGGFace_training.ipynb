{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from math import ceil, floor\n",
    "import itertools\n",
    "import datetime\n",
    "import os\n",
    "import os.path as osp\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torchfile\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, metrics, model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFace, self).__init__()\n",
    "\n",
    "        self.features = nn.ModuleDict(OrderedDict(\n",
    "            {\n",
    "                # === Block 1 ===\n",
    "                'conv_1_1': nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "                'relu_1_1': nn.ReLU(inplace=True),\n",
    "                'conv_1_2': nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                'relu_1_2': nn.ReLU(inplace=True),\n",
    "                'maxp_1_2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                # === Block 2 ===\n",
    "                'conv_2_1': nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "                'relu_2_1': nn.ReLU(inplace=True),\n",
    "                'conv_2_2': nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "                'relu_2_2': nn.ReLU(inplace=True),\n",
    "                'maxp_2_2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                # === Block 3 ===\n",
    "                'conv_3_1': nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "                'relu_3_1': nn.ReLU(inplace=True),\n",
    "                'conv_3_2': nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                'relu_3_2': nn.ReLU(inplace=True),\n",
    "                'conv_3_3': nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "                'relu_3_3': nn.ReLU(inplace=True),\n",
    "                'maxp_3_3': nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "                # === Block 4 ===\n",
    "                'conv_4_1': nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "                'relu_4_1': nn.ReLU(inplace=True),\n",
    "                'conv_4_2': nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                'relu_4_2': nn.ReLU(inplace=True),\n",
    "                'conv_4_3': nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                'relu_4_3': nn.ReLU(inplace=True),\n",
    "                'maxp_4_3': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                # === Block 5 ===\n",
    "                'conv_5_1': nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                'relu_5_1': nn.ReLU(inplace=True),\n",
    "                'conv_5_2': nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                'relu_5_2': nn.ReLU(inplace=True),\n",
    "                'conv_5_3': nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                'relu_5_3': nn.ReLU(inplace=True),\n",
    "                'maxp_5_3': nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            }))\n",
    "\n",
    "        self.fc = nn.ModuleDict(OrderedDict(\n",
    "            {\n",
    "                'fc6': nn.Linear(in_features=512 * 7 * 7, out_features=4096),\n",
    "                'fc6-relu': nn.ReLU(inplace=True),\n",
    "                'fc6-dropout': nn.Dropout(p=0.5),\n",
    "                'fc7': nn.Linear(in_features=4096, out_features=4096),\n",
    "                'fc7-relu': nn.ReLU(inplace=True),\n",
    "                'fc7-dropout': nn.Dropout(p=0.5),\n",
    "                'fc8': nn.Linear(in_features=4096, out_features=2622),\n",
    "            }))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through feature layers\n",
    "        for k, layer in self.features.items():\n",
    "            x = layer(x)\n",
    "\n",
    "        # Flatten convolution outputs\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward through FC layers\n",
    "        for k, layer in self.fc.items():\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGFace training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create embeddings for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model, dataloader: data.DataLoader, path_embed: str, path_labels: str):\n",
    "    \"\"\" Creates embeddings and labels to files\n",
    "\n",
    "    Args:\n",
    "        model: Neural model\n",
    "        dataloader: Dataloader\n",
    "        path_embed: Path to embeddings to save\n",
    "        path_labels: Path to labels to save\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        data, label = batch\n",
    "        embeddings.append(*model(data).tolist())\n",
    "        labels.append(*label.tolist())\n",
    "    np.save(path_embed, embeddings)\n",
    "    np.save(path_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extracting_model():\n",
    "    \"\"\" Get feature-extracting model\n",
    "\n",
    "    Returns:\n",
    "        Feature-extracting model\n",
    "    \"\"\"\n",
    "    # Build VGGFace model and load pre-trained weights\n",
    "    model = VGGFace()\n",
    "    model_dict = torch.load('models/vggface.pth', map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(model_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # Fine-tuning\n",
    "    model.fc['fc7-relu'] = nn.Identity()\n",
    "    model.fc['fc7-dropout'] = nn.Identity()\n",
    "    model.fc['fc8'] = nn.Identity()\n",
    "\n",
    "    # Freezing weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGFace(\n",
      "  (features): ModuleDict(\n",
      "    (conv_1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_1_1): ReLU(inplace=True)\n",
      "    (conv_1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_1_2): ReLU(inplace=True)\n",
      "    (maxp_1_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv_2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_2_1): ReLU(inplace=True)\n",
      "    (conv_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_2_2): ReLU(inplace=True)\n",
      "    (maxp_2_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv_3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_3_1): ReLU(inplace=True)\n",
      "    (conv_3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_3_2): ReLU(inplace=True)\n",
      "    (conv_3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_3_3): ReLU(inplace=True)\n",
      "    (maxp_3_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv_4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_4_1): ReLU(inplace=True)\n",
      "    (conv_4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_4_2): ReLU(inplace=True)\n",
      "    (conv_4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_4_3): ReLU(inplace=True)\n",
      "    (maxp_4_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv_5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_5_1): ReLU(inplace=True)\n",
      "    (conv_5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_5_2): ReLU(inplace=True)\n",
      "    (conv_5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu_5_3): ReLU(inplace=True)\n",
      "    (maxp_5_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): ModuleDict(\n",
      "    (fc6): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (fc6-relu): ReLU(inplace=True)\n",
      "    (fc6-dropout): Dropout(p=0.5, inplace=False)\n",
      "    (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (fc7-relu): Identity()\n",
      "    (fc7-dropout): Identity()\n",
      "    (fc8): Identity()\n",
      "  )\n",
      ")\n",
      "Getting dataloaders...\n",
      "Done!\n",
      "Create embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 926/926 [04:37<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "model = get_feature_extracting_model()\n",
    "print(model)\n",
    "print('Getting dataloaders...')\n",
    "# Dataset must be located in folder \"dataset\"\n",
    "dataset = datasets.ImageFolder('dataset', transform=transforms.Compose([transforms.ToTensor()]))\n",
    "dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "print('Done!')\n",
    "\n",
    "print('Create embeddings...')\n",
    "model.eval()\n",
    "# Embeddings and labels will be stored in folder \"embeds\"\n",
    "create_embeddings(model, dataloader, 'embeds/embeds.npy', 'embeds/labels.npy')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a SVM classifier and calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create train and test samples\n",
    "embeds = np.load('embeds/embeds.npy')\n",
    "labels = np.load('embeds/labels.npy')\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(embeds, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM classifier\n",
    "classifier = svm.SVC(C=1, probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "pred = classifier.predict(X_test)\n",
    "proba = classifier.predict_proba(X_test)\n",
    "print('Predictions:')\n",
    "print(pred)\n",
    "print('Ground truth:')\n",
    "print(y_test)\n",
    "\n",
    "# Show metrics\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, proba[:,0], pos_label=0)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print('roc_auc: ', roc_auc)\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print('acc: ', acc)\n",
    "f1 = metrics.f1_score(y_test, pred, average='binary')\n",
    "print('f1: ', f1)\n",
    "\n",
    "# Plot ROC-AUC curve\n",
    "plt.style.use('seaborn')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define the device where we will train on\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                    epoch: int, history: list, best_val_acc: float):\n",
    "    \"\"\" Saves checkpoint to .pth file in /checkpoints directory\n",
    "\n",
    "    Args:\n",
    "        model: Neural model\n",
    "        optimizer: Optimizer (Exmpl: Adam)\n",
    "        epoch: Epochs amount\n",
    "        history: List of model metrics\n",
    "        best_val_acc: Best accuracy on validation set\n",
    "    \"\"\"\n",
    "    dt = datetime.datetime.now()\n",
    "    dt_string = dt.strftime(\"%d-%m-%Y %H-%M\")\n",
    "    pathname_pattern = f\"checkpoints/{dt_string} {epoch}-epoch.pth\"\n",
    "    with open(pathname_pattern, 'wb') as f:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'history': history,\n",
    "            'best_val_acc': best_val_acc\n",
    "        }, f)\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, model: nn.Module, optimizer: torch.optim.Optimizer) -> tuple[int, list, float]:\n",
    "    \"\"\" Loads checkpoint from .pth file. Configures model and optimizer\n",
    "\n",
    "    Args:\n",
    "        path: Path to checkpoint\n",
    "        model: Neural model (Same that used in saved checkpoint)\n",
    "        optimizer: Optimizer (Same that used in saved checkpoint)\n",
    "\n",
    "    Returns:\n",
    "        List of number of epochs, history, best validation accuracy\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        checkpoint = torch.load(f)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    history = checkpoint['history']\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    return epoch, history, best_val_acc\n",
    "\n",
    "\n",
    "def epoch_fit(model: torch.nn.Module,\n",
    "              train_dataloader: data.DataLoader,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion: torch.nn.CrossEntropyLoss) -> tuple[float, float]:\n",
    "    \"\"\" Training model on 1 epoch\n",
    "\n",
    "    Args:\n",
    "        model: Neural model\n",
    "        train_dataloader: Train dataloader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "\n",
    "    Returns:\n",
    "        List of epoch loss, epoch accuracy\n",
    "    \"\"\"\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    num_of_batches = len(train_dataloader)\n",
    "    num_of_elems = len(train_dataloader.dataset)\n",
    "\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Send tensors to DEVICE\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Reset gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Backward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        epoch_acc += sum(preds == labels.data)\n",
    "\n",
    "    # Calculate average values\n",
    "    epoch_loss /= num_of_batches\n",
    "    epoch_acc /= num_of_elems\n",
    "    return epoch_loss, epoch_acc.cpu()\n",
    "\n",
    "\n",
    "def epoch_eval(model: nn.Module,\n",
    "               val_dataloader: data.DataLoader,\n",
    "               criterion: torch.nn.CrossEntropyLoss) -> tuple[float, float]:\n",
    "    \"\"\" Validate model on 1 epoch\n",
    "\n",
    "    Args:\n",
    "        model: Neural model\n",
    "        val_dataloader: Validation dataloader\n",
    "        criterion: Loss function\n",
    "\n",
    "    Returns:\n",
    "        List of validation loss, validation accuracy\n",
    "    \"\"\"\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    num_of_batches = len(val_dataloader)\n",
    "    num_of_elems = len(val_dataloader.dataset)\n",
    "\n",
    "    for inputs, labels in val_dataloader:\n",
    "        # Send tensors to DEVICE\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Getting model outputs without calculating gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss and accuracy\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            val_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Calculate average values\n",
    "    val_loss /= num_of_batches\n",
    "    val_acc /= num_of_elems\n",
    "    return val_loss, val_acc.cpu()\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          train_dataloader: data.DataLoader,\n",
    "          val_dataloader: data.DataLoader,\n",
    "          epochs: int,\n",
    "          checkpoint_path: str = None) -> list:\n",
    "    \"\"\" Train loop\n",
    "        Saves weights of best model in \"models/best_model.pth\"\n",
    "        \n",
    "    Args:\n",
    "        model: Neural model\n",
    "        train_dataloader: Train dataloader\n",
    "        val_dataloader: Validation dataloader\n",
    "        epochs: Number of epochs\n",
    "        checkpoint_path: Path to checkpoint to continue from\n",
    "\n",
    "    Returns:\n",
    "        Matrix of metrics and loss on train and val sets for each epoch: [train_loss, train_acc, val_loss, val_acc]\n",
    "    \"\"\"\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Define training parameters\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.01, amsgrad=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    history = []\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "                    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    # Load checkpoint\n",
    "    if checkpoint_path is not None:\n",
    "        start_epoch, history, best_val_acc = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "        print(f'Checkpoint {checkpoint_path} loaded!')\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in tqdm(range(start_epoch, epochs), desc='Epoch', position=0, leave=True):\n",
    "        # Training\n",
    "        train_loss, train_acc = epoch_fit(model, train_dataloader, optimizer, criterion)\n",
    "        # Validation\n",
    "        val_loss, val_acc = epoch_eval(model, val_dataloader, criterion)\n",
    "        history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        # Create checkpoint every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            save_checkpoint(model, optimizer, epoch, history, best_val_acc)\n",
    "\n",
    "        # Saves best model\n",
    "        if best_val_acc < val_acc:\n",
    "            torch.save(model.state_dict(), \"models/best_model.pth\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        tqdm.write(log_template.format(ep=epoch, t_loss=train_loss,\n",
    "                                       v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "    return history\n",
    "\n",
    "\n",
    "def test(model: nn.Module, test_dataloader: data.DataLoader, with_plot: bool = True) -> tuple[float, float, float]:\n",
    "    \"\"\" Testing model\n",
    "\n",
    "    Args:\n",
    "        model: Neural model\n",
    "        test_dataloader: Test dataloader\n",
    "        with_plot: True - plot a ROC-AUC curve\n",
    "\n",
    "    Returns:\n",
    "        List of metrics: [roc_auc, acc, f1]\n",
    "    \"\"\"\n",
    "    # Switch model to evaluating mode\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "\n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "        pred_labels = []\n",
    "        for inputs, labels in tqdm(test_dataloader):\n",
    "            # Send tensors to DEVICE\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs).cpu()\n",
    "\n",
    "            # Accumulate logits\n",
    "            logits.append(outputs)\n",
    "            y_true = list(itertools.chain(y_true, labels))\n",
    "            pred_labels.append(*torch.argmax(outputs, dim=1).data)\n",
    "\n",
    "    # Get model probability predictions\n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    pred_probs = probs[:, 0]\n",
    "\n",
    "    # Getting metrics\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, pred_probs, pos_label=0)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    acc = metrics.accuracy_score(y_true, pred_labels)\n",
    "    f1 = metrics.f1_score(y_true, pred_labels, average='binary')\n",
    "\n",
    "    # Plot ROC-AUC curve\n",
    "    if with_plot:\n",
    "        plt.style.use('seaborn')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "\n",
    "    return roc_auc, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_fine_tuned_model():\n",
    "    \"\"\" Get fine-tuned model\n",
    "\n",
    "    Returns:\n",
    "        Pretrained fine-tuned model\n",
    "    \"\"\"\n",
    "    # Build VGGFace model and load pre-trained weights\n",
    "    model = VGGFace()\n",
    "    model_dict = torch.load('models/vggface.pth', map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(model_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # Freezing weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Fine-tuning\n",
    "    model.fc['fc8'] = nn.Linear(in_features=4096, out_features=2)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_datasets(path: str, transform: transforms = None, with_val: bool = False, seed: int = None):\n",
    "    \"\"\" Get train, test and val dataset\n",
    "\n",
    "        Args:\n",
    "            path: Path to dataset\n",
    "            transform: Dataset transformation (augmentation)\n",
    "            with_val: Divide dataset into train, test, val with \"True\" value.\n",
    "                      Otherwise return only train and test\n",
    "            seed: Seed for splitting\n",
    "\n",
    "        Returns:\n",
    "            Divided dataset into train, test, (val)\n",
    "    \"\"\"\n",
    "    general_dataset = datasets.ImageFolder(path, transform=transform)\n",
    "    dataset_size = len(general_dataset)\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    if with_val: # Train, test, val\n",
    "        train_dataset, test_val_dataset = data.random_split(general_dataset,\n",
    "                                                        (ceil(dataset_size * 0.6), floor(dataset_size * 0.4)))\n",
    "        test_val_size = len(test_val_dataset)\n",
    "        test_dataset, val_dataset = data.random_split(test_val_dataset, (ceil(test_val_size * 0.5),\n",
    "                                                                        floor(test_val_size * 0.5)))\n",
    "        return train_dataset, test_dataset, val_dataset\n",
    "    else:   # Train, test\n",
    "        train_dataset, test_dataset = data.random_split(general_dataset,\n",
    "                                                        (ceil(dataset_size * 0.8), floor(dataset_size * 0.2)))\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    \n",
    "def history_plot(history: list):\n",
    "    \"\"\" Plots a history graph\n",
    "\n",
    "    Args:\n",
    "        history: List of values consisted of loss, accuracy\n",
    "                 on train and val samples\n",
    "    \"\"\"\n",
    "    loss, acc, val_loss, val_acc = zip(*history)\n",
    "\n",
    "    plt.style.use('seaborn')\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "\n",
    "    ax1.plot(loss, label=\"train loss\")\n",
    "    ax1.plot(val_loss, label=\"val loss\")\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax2.plot(acc, label='train accuracy')\n",
    "    ax2.plot(val_acc, label='val accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'Train on {DEVICE}')\n",
    "batch_size = 20\n",
    "print(f'Batch size: {batch_size}')\n",
    "\n",
    "# Getting train, test, val datasets\n",
    "train_dataset, test_dataset, val_dataset = get_datasets('dataset',\n",
    "                                                        transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                                        with_val=True,\n",
    "                                                        seed=42)\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, shuffle=False)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Getting model\n",
    "model = get_fine_tuned_model()\n",
    "print(model, '\\n')\n",
    "print(f'All layers: {len(model.state_dict())}')\n",
    "print('Activated layers:')\n",
    "for i, layer in enumerate(model.parameters(), start=1):\n",
    "    if layer.requires_grad:\n",
    "        print(i, layer.requires_grad)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "model.to(DEVICE)\n",
    "history = train(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                epochs=51,\n",
    "                checkpoint_path=None)\n",
    "np.save('history.npy', history)     # Save loss and accuracy\n",
    "\n",
    "# Plotting history\n",
    "# history = np.load('history.npy')\n",
    "history_plot(history)\n",
    "\n",
    "# Load weights of best fine-tuned model\n",
    "# with open('models/best_model.pth', 'rb') as f:\n",
    "#     dictw = torch.load(f)\n",
    "#     model.load_state_dict(dictw)\n",
    "\n",
    "# Calculate metrics\n",
    "print('\\nCalculate metrics:')\n",
    "model.to(DEVICE)\n",
    "roc_auc, acc, f1_score = test(model, test_dataloader)\n",
    "print(f'Roc-auc: {roc_auc}')\n",
    "print(f'Accuracy: {acc}')\n",
    "print(f'F1-score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def ft_cross_val(general_dataset: data.Dataset, epochs: int, batch_size: int):\n",
    "    \"\"\" Cross-validation of fine-tuned model\n",
    "\n",
    "    Args:\n",
    "        general_dataset: Dataset\n",
    "        epochs: Amount of epochs\n",
    "        batch_size: Size of batch\n",
    "\n",
    "    Returns:\n",
    "        List of model metrics on each fold: [roc-auc, acc, f1]\n",
    "    \"\"\"\n",
    "    # Divide into folds\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    results = []\n",
    "\n",
    "    # Train and validate ft_model on folds\n",
    "    for fold, (train_datasplit, test_datasplit) in enumerate(kfold.split(general_dataset)):\n",
    "        print(f'FOLD #{fold}')\n",
    "        train_dataloader = data.DataLoader(general_dataset, batch_size, shuffle=False, sampler=train_datasplit)\n",
    "        test_dataloader = data.DataLoader(general_dataset, 1, shuffle=False, sampler=test_datasplit)\n",
    "\n",
    "        model = get_fine_tuned_model()\n",
    "        model.to(DEVICE)\n",
    "        train(model, train_dataloader, test_dataloader, epochs)\n",
    "        # with open('models/best_model.pth', 'rb') as f:\n",
    "        #     dictw = torch.load(f)\n",
    "        #     model.load_state_dict(dictw)\n",
    "        roc_auc, acc, f1 = test(model, test_dataloader, False)\n",
    "        print(f'ROC AUC: {roc_auc}, Acc: {acc}, F1: {f1}')\n",
    "        results.append([roc_auc, acc, f1])\n",
    "\n",
    "    print('All scores: ', results)\n",
    "    average = np.average(results, axis=0)\n",
    "    print(f'K-fold CV average result.\\nROC-AUC: {average[0]}, Acc: {average[1]}, F1: {average[2]}')\n",
    "    return results\n",
    "\n",
    "\n",
    "def svm_cross_val(X_src: str, y_src: str):\n",
    "    \"\"\" Cross-validation of SVM model\n",
    "\n",
    "    Args:\n",
    "        X_src: Path to embeddings\n",
    "        y_src: Path to labels\n",
    "\n",
    "    Returns:\n",
    "        List of model metrics on each fold: [roc-auc, acc, f1]\n",
    "    \"\"\"\n",
    "    X = np.load(X_src)\n",
    "    y = np.load(y_src)\n",
    "    print(f'Size of general dataset: {X.shape}')\n",
    "    classifier = SVC(C=0.3)\n",
    "    scoring = ['roc_auc', 'accuracy', 'f1']\n",
    "    result = cross_validate(classifier, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
    "    return result\n",
    "\n",
    "\n",
    "def print_cv_res(cv_res_path: str):\n",
    "    \"\"\" Prints average result of cross-validation\n",
    "\n",
    "    Args:\n",
    "        cv_res_path: Path to list of model metrics\n",
    "    \"\"\"\n",
    "    ft_cv_res = np.load(cv_res_path)\n",
    "    print('All scores: ', ft_cv_res)\n",
    "    average = np.average(ft_cv_res, axis=0)\n",
    "    print(average.shape)\n",
    "    print(f'K-fold CV average result.\\nROC-AUC: {average[0]}, Acc: {average[1]}, F1: {average[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVM CV ---\n",
      "Size of general dataset: (926, 4096)\n",
      "{'fit_time': array([1.64290166, 1.32257819, 1.35756707, 1.35156679, 1.37156034]), 'score_time': array([1.18165827, 1.10364413, 1.08065391, 1.11464405, 1.12863636]), 'test_roc_auc': array([0.80209661, 0.65967666, 0.69218165, 0.75386236, 0.80348783]), 'test_accuracy': array([0.70967742, 0.64864865, 0.65405405, 0.71351351, 0.72432432]), 'test_f1': array([0.67857143, 0.67005076, 0.65591398, 0.70056497, 0.68711656])}\n",
      "K-fold CV average result.\n",
      "ROC-AUC: 0.7422610209590401, Acc: 0.6900435919790758, F1: 0.6784435409311925\n"
     ]
    }
   ],
   "source": [
    "print('--- SVM CV ---')\n",
    "result = svm_cross_val('embeds/embeds.npy', 'embeds/labels.npy')\n",
    "print(result)\n",
    "roc_auc, acc, f1 = result['test_roc_auc'], result['test_accuracy'], result['test_f1']\n",
    "np.save('svm_cv_res.npy', [roc_auc, acc, f1])\n",
    "print(f'K-fold CV average result.\\nROC-AUC: {np.average(roc_auc)}, Acc: {np.average(acc)}, F1: {np.average(f1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-tuned CV ---\n",
      "Train on cuda:0\n",
      "FOLD #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 1/30 [00:25<12:19, 25.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 000 train_loss: 0.7414                     val_loss 0.6845 train_acc 0.4104 val_acc 0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 1/30 [00:28<13:44, 28.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([transforms\u001B[38;5;241m.\u001B[39mToTensor()])\n\u001B[0;32m      4\u001B[0m general_dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mImageFolder(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[1;32m----> 5\u001B[0m ft_cv_res \u001B[38;5;241m=\u001B[39m \u001B[43mft_cross_val\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneral_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m np\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mft_cv_res_30_epochs.npy\u001B[39m\u001B[38;5;124m'\u001B[39m, ft_cv_res)\n",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36mft_cross_val\u001B[1;34m(general_dataset, epochs, batch_size)\u001B[0m\n\u001B[0;32m     22\u001B[0m model \u001B[38;5;241m=\u001B[39m get_fine_tuned_model()\n\u001B[0;32m     23\u001B[0m model\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m---> 24\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# with open('models/best_model.pth', 'rb') as f:\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m#     dictw = torch.load(f)\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m#     model.load_state_dict(dictw)\u001B[39;00m\n\u001B[0;32m     28\u001B[0m roc_auc, acc, f1 \u001B[38;5;241m=\u001B[39m test(model, test_dataloader, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, val_dataloader, epochs, checkpoint_path)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# Train loop\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(start_epoch, epochs), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch\u001B[39m\u001B[38;5;124m'\u001B[39m, position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;66;03m# Training\u001B[39;00m\n\u001B[1;32m--> 178\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mepoch_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[0;32m    180\u001B[0m     val_loss, val_acc \u001B[38;5;241m=\u001B[39m epoch_eval(model, val_dataloader, criterion)\n",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36mepoch_fit\u001B[1;34m(model, train_dataloader, optimizer, criterion)\u001B[0m\n\u001B[0;32m     85\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# Accumulate loss and accuracy\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     90\u001B[0m epoch_acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(preds \u001B[38;5;241m==\u001B[39m labels\u001B[38;5;241m.\u001B[39mdata)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print('--- Fine-tuned CV ---')\n",
    "print(f'Train on {DEVICE}')\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "general_dataset = datasets.ImageFolder('dataset', transform=transform)\n",
    "ft_cv_res = ft_cross_val(general_dataset, 30, 16)\n",
    "np.save('ft_cv_res_30_epochs.npy', ft_cv_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}